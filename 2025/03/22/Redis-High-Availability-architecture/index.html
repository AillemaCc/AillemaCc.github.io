<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Redis-High-Availability-architecture | AllimacBlog</title><meta name="author" content="Allimac"><meta name="copyright" content="Allimac"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="高可用架构实现">
<meta property="og:type" content="article">
<meta property="og:title" content="Redis-High-Availability-architecture">
<meta property="og:url" content="http://example.com/2025/03/22/Redis-High-Availability-architecture/index.html">
<meta property="og:site_name" content="AllimacBlog">
<meta property="og:description" content="高可用架构实现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/avatar.png">
<meta property="article:published_time" content="2025-03-22T10:22:31.000Z">
<meta property="article:modified_time" content="2025-03-22T10:26:01.429Z">
<meta property="article:author" content="Allimac">
<meta property="article:tag" content="八股">
<meta property="article:tag" content="Redis">
<meta property="article:tag" content="高可用">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Redis-High-Availability-architecture",
  "url": "http://example.com/2025/03/22/Redis-High-Availability-architecture/",
  "image": "http://example.com/img/avatar.png",
  "datePublished": "2025-03-22T10:22:31.000Z",
  "dateModified": "2025-03-22T10:26:01.429Z",
  "author": [
    {
      "@type": "Person",
      "name": "Allimac",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/faviconcute.png"><link rel="canonical" href="http://example.com/2025/03/22/Redis-High-Availability-architecture/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Redis-High-Availability-architecture',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/nature_top_image.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">AllimacBlog</span></a><a class="nav-page-title" href="/"><span class="site-name">Redis-High-Availability-architecture</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Redis-High-Availability-architecture</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-03-22T10:22:31.000Z" title="发表于 2025-03-22 18:22:31">2025-03-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-22T10:26:01.429Z" title="更新于 2025-03-22 18:26:01">2025-03-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">12.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>36分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="redis如何实现高可用架构"><a href="#redis如何实现高可用架构" class="headerlink" title="redis如何实现高可用架构"></a>redis如何实现高可用架构</h1><p>Redis 官方提供了三种分布式部署模式：</p>
<ul>
<li><strong>主从模式</strong>：通过 <code>slaveof</code> 命令可以让任意的节点成为对应的节点的从节点，主节点通过主从复制机制向从主节点同步数据。其中，主节点提供读写功能，而从节点只提供读功能。当主节点出现故障时，需要手动进行主从切换完成故障转移。</li>
<li><strong>Sentinel 哨兵模式</strong>：哨兵模式是主从的增强版本，它在主从模式的基础上添加了哨兵节点，哨兵节点通过心跳监控节点健康状态，并在主节点出现故障后，自动通过投票机制选举出新的主节点并完成故障转移。不过哨兵本身也需要做保证高可用，因此一般也会同时部署多个哨兵节点组成哨兵集群。</li>
<li><strong>Cluster 集群模式</strong>：Redis Cluster 一般也叫分片集群，它是 Redis 提供的一个去中心化的集群部署方案。它将全部数据划分为 16394  个哈希槽，由集群中的每个节点负责其中的一部分槽位，使用时 Key 将通过哈希取模并最终指派到某个特定的槽位。在集群中，不同的 Redis  节点间通过 Gossip  协议与其他节点保持通信，每个主节点都将可以具备多个从节点，从节点不提供服务，当主节点出现故障时将会自动投票选举出新的主节点，并且完成故障转移。</li>
</ul>
<p>此外，在 Redis Cluster 之前，也有一些基于中间代理路由或客户端路由的 Redis 的解决方案，比如 Jedis 自带的 ShardedJedis，或者推特的 Twemproxy。</p>
<h2 id="主从模式"><a href="#主从模式" class="headerlink" title="主从模式"></a>主从模式</h2><p>当 Redis 的访问量以及数据量随着随着业务规模一起扩大，单机部署的一些问题就逐渐体现出来，比如：</p>
<ul>
<li>服务器一旦宕机，所有 Redis 服务都不可用。</li>
<li>读写请求全打到单个 Redis 实例上，会遇到性能瓶颈。</li>
</ul>
<p>为了解决这些问题，Redis 中提供了主从复制（replication）的机制。简单的来说，当我们有多个 Redis 实例时，它们会被划分为<strong>两类节点</strong>：</p>
<ul>
<li>Master 主节点：负责处理客户端的读写请求，以及数据的修改操作。</li>
<li>Slave 从节点：通过复制主节点的数据，提供备份和读取服务。</li>
</ul>
<p>一个主节点加一个从节点就可以构成一个最小规模的主从结构，其中，从节点还可以再拥有自己的从节点，从而更进一步的提高容灾能力。</p>
<p>相对单机部署，主从模式的<strong>优点</strong>十分的明显：</p>
<ul>
<li>配置和使用非常简单。</li>
<li>可以做到读写分离，提高了性能。</li>
<li>提供了容灾备份能力。</li>
</ul>
<p>不过它也有<strong>不足之处</strong>：</p>
<ul>
<li>没有自动故障转移，主节点故障时需要手动切换，操作起来比较麻烦。</li>
<li>主节点需要承担所有的写入操作，还是会面临性能瓶颈。</li>
<li>数据没有做到分片存储，因此依然要面对单机数据容量限制的问题。</li>
</ul>
<h2 id="sentinel模式"><a href="#sentinel模式" class="headerlink" title="sentinel模式"></a>sentinel模式</h2><p>Sentinel 哨兵模式是 Redis 提供的一种用于监控和管理 Redis 主从节点的机制。</p>
<p>顾名思义，它在主从节点的基础上额外增加了一类哨兵节点，它是一个特殊的 Redis 实例，会定期向主节点和从节点发送心跳检测。</p>
<p>当检测到主节点不可用时，哨兵会自动通过选举机制，将一个从节点升级为新的主节点，并且更新所有其他从节点的配置，使它们成为新主节点的从节点。</p>
<p>不过，哨兵模式同样要面临单点故障问题，因此为了保证哨兵节点的高可用，我们往往需要部署多个哨兵节点让他们组成哨兵集群。</p>
<p><strong>哨兵模式弥补了普通的主从模式不支持自动故障转移的缺点</strong>，不过它的配置较复杂，管理起来也不方便，最重要的是，除此之外，它依然<strong>没有解决读写请求全部打到主节点带来的性能问题，以及单机部署造成的数据容量限制的问题</strong>。</p>
<h2 id="cluster模式"><a href="#cluster模式" class="headerlink" title="cluster模式"></a>cluster模式</h2><p>上面两种方案实际上都只做到了“多从”，而如果想要提高写入性能，并突破单机数据容量的限制，那还需要做到“多主”，对此， Redis 提供了分片集群（Cluster）作为解决方案。</p>
<p>在 Redis 集群中，划分了 16384（2^14）个哈希槽，所有的 Key 在计算后，都必定会落到这些槽位中的任意一个，集群中的每个 Redis 实例都将管理其中一部分的槽位，<strong>每个节点与它负责的哈希槽，我们一般称其为“分片”</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd3xbD"><img src="https://s21.ax1x.com/2025/03/17/pEd3xbD.png" alt="pEd3xbD.png"></a></p>
<p>当客户端向某个 Redis 节点发起请求时，将会通过哈希函数计算当前请求的 Key 落在哪一个分片，如果该 Key  不属于这个实例管理的分片，就会被转发到对应的分片上的 Redis 节点。 其中，分片的大小不一定要一致，如果有的 Redis  实例性能强一点，或者内存多一些，就可以分配给他更多的槽位。</p>
<p>这种方案中的 Redis 实例同样可以有多个从节点，不过它们只单纯作为备份，而不像传统的主从架构那样提供读服务。</p>
<p>它的特点是：</p>
<ul>
<li><strong>去中心化</strong>：整个集群中的所有节点 redis 节点都彼此直接通信，每个节点都存储了集群的哈希槽分配信息。</li>
<li><strong>客户端直连</strong>：客户端不需要任何代理层，连接上集群任意一个节点即可访问整个集群的数据。</li>
<li><strong>自动故障转移</strong>：集群中任意节点下线以后，将会通过投票完成主从切换。</li>
</ul>
<p>分片集群在保证高可用的前提下，实现了数据分片存储与负载均衡，不过它也有对应的缺点：</p>
<ul>
<li><strong>管理难度高</strong>：由于是去中心化集群，因此你无法集中的管理节点，尤其是在扩缩容或者故障转移这种场景下，更是很难预测具体的行为。</li>
<li><strong>客户端实现较复杂</strong>：为了避免多次转发带来的性能问题，因此客户端需要考虑缓存分片配置，同时还要考虑到集群状态调整后缓存数据一致性的问题……</li>
<li><strong>不支持批量操作和事务</strong>：批量操作的 Key 如果不是全部属于同一个分片，就需要分成多次请求完成，因此也就无法保证 Redis 的事务。</li>
</ul>
<h2 id="其他方案"><a href="#其他方案" class="headerlink" title="其他方案"></a>其他方案</h2><p>有意思的是，由于 Redis 官方的集群方案出现的比较晚，因此在它之前，行业内已经出现一些其它的解决方案，不过大体都是通过哈希算法对 Key 取模，从而将对 Key 的请求转发到集群中的各个节点。</p>
<p>总的来说，相比起 Redis Cluster，它们的主要区别在进行路由的层级不同，而这些方案大体可以分为两类：</p>
<ul>
<li><strong>客户端路由</strong>：直接在客户端就完成 Key 的路由。它最大的优点是方便，完全不需要引入其他的中间层，不过对应的，面对集群进行故障转移或动态扩缩容的情况就会比较难以处理。</li>
<li><strong>代理层路由</strong>：在客户端和 Redis  集群加一个中间的代理层，在代理层的中间件完成路由。这个方案的优点是代理层的中间件本身能起到管理者的作用，因此它能更好的管理集群，并且在面对故障转移和动态扩缩容时能更加快速的作出响应。它最大缺点是成本较高，因为代理层本身也是需要维护成本，并且为了保障高可用同样需要进行集群部署。一般各大公有云商会考虑采用这种方案。</li>
</ul>
<h1 id="主从复制原理"><a href="#主从复制原理" class="headerlink" title="主从复制原理"></a>主从复制原理</h1><p><strong>当从节点初次连接到主节点，或者掉线重连后进度落后较多时会进行一次全量数据同步</strong>。此时，主节点会生成 RDB 快照并传输给从节点，在此期间，主节点接受到的增量命令将会先写入 <code>replication_buffer</code> 缓冲区，等到从节点加载完 RDB 快照的数据后，再将缓冲区的命令传输给从节点，以次完成初次同步。</p>
<p><strong>当从节点掉线重连后，如果进度落后的不多，将会进行增量同步</strong>。主节点内部维护了一个环形的固定大小的 <code>repl_backlog_buffer</code> 缓冲区，它用于记录最近传播的命令。其中，主节点和从节点会分别在该缓冲区维护一个 offset  ，用于表示自己的写进度和读进度。当从节点掉线重连后，将会检查主节点和从节点 offset  之差是否小于缓冲区大小，如果确实小于，说明从节点同步进度落后不多，则主节点将该缓冲区中的两 offset  之间的增量命令发送给从节点，完成增量同步。</p>
<p><strong>当主从节点完成初次同步后，将会建立长连接进行命令传播</strong>。简单的来说，就是每当主节点执行一条命令，它就会写入 <code>replication_buffer</code> 缓冲区，随后再将缓冲区的命令通过节点间的长连接发送给对应的从节点。</p>
<h2 id="全量同步"><a href="#全量同步" class="headerlink" title="全量同步"></a>全量同步</h2><p>一般来说，当两个节点第一次建立主从关系的时候，一定会触发一次全量同步。整个过程大致分为四步：</p>
<ol>
<li>从节点向主节点发送 psync 请求获取 runID 和 offset；</li>
<li>主节点返回自己的 runID 与该从节点的 offset；</li>
<li>主节点生成 RDB 文件，并传输给从节点，从节点加载文件后获取全量数据。</li>
<li>主节点确认从节点加载完 RDB 文件后，将这期间缓存的增量命令发送给从节点，从节点加载完毕后结束第一次同步。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd8ZqS"><img src="https://s21.ax1x.com/2025/03/17/pEd8ZqS.png" alt="pEd8ZqS.png"></a></p>
<h3 id="获取runid-offset"><a href="#获取runid-offset" class="headerlink" title="获取runid offset"></a>获取runid offset</h3><p>当我们对一个 Redis 实例使用 <code>replicaof</code> 让指定从节点与主节点构成主从关系时，从节点将会根据你指定的 host 和端口号请求对应的服务实例，并向主节点发送 <code>psync &#123;runID&#125; &#123;offset&#125;</code> 请求。</p>
<p>其中，runID 是主节点启动时生成的标识 ID，而 offset 则表示当前从节点从主节点复制数据的偏移量，也就是复制进度。最开始的时候，由于是第一次同步，因此从节点并不知道主节点的 runID，因此 runID 为 <code>?</code>，由于也还没有复制过数据，因此 offset 为 -1。</p>
<p>当主节点响应请求时，将会返回主节点自己的 runID 和当前从节点的复制进度 offset。并且因为是首次同步，因此主节点会返回 <code>FULLRESYNC</code> 响应，表示从节点需要<strong>进行全量同步</strong>。</p>
<h3 id="加载RDB数据"><a href="#加载RDB数据" class="headerlink" title="加载RDB数据"></a>加载RDB数据</h3><p>在前文 Redis 的持久化部分，我们提到过，Redis 恢复数据或者进行主从同步的时候是通过 RDB 文件完成的，实际上指的就是全量同步。</p>
<p>在从节点接受了响应以后，主节点将会执行 bgsave 去生成一个 RDB 文件——这个流程与正常生成 RDB 文件一致——并发送给从节点，从节点接受后，由于是全量同步，因此会<strong>先清空自己的已有数据，然后再加载 RDB 文件中的数据</strong>。</p>
<p>此时，从节点已经同步过来了主节点的大部分数据，不过由于 RDB 实际上只是一个快照，因此在<strong>生成 RDB 文件期间主节点的增量数据实际上还没有被从节点获取</strong>。</p>
<h3 id="获取增量数据"><a href="#获取增量数据" class="headerlink" title="获取增量数据"></a>获取增量数据</h3><p>在主节点生成 RDB 文件期间，由于 RDB 文件是通过子进程异步生成的，因此在这个过程中主节点仍然还在正常的处理请求，这部分的<strong>增量命令将会写入 replication buffer 缓冲区</strong>。</p>
<p>当从节点加载完 RDB 中的数据后，将会向主节点发送确认消息，此时主节点会再将缓冲区中的命令发送给从节点，从节点执行完这部分增量命令后，数据即与主节点基本一致，则全量同步完成。</p>
<p>不过，由于主从的更新已有延迟，因此无论如何数据是很难保证完全一致的，不过这就是后面命令的正常传播和增量同步的事情了。</p>
<h2 id="命令传播"><a href="#命令传播" class="headerlink" title="命令传播"></a>命令传播</h2><p>当主从之间根据 <code>psync</code> 请求完成第一次全量或增量同步后，就会保持长连接，此后，将会通过长连接进行命令传播。</p>
<p>在这个过程中，主节点将会在每次执行完一个命令后，分别写两个缓冲区：</p>
<ul>
<li><strong>replication_buffer</strong>：主从复制缓冲区，主节点每拥有一个从节点，就会有一个对应的缓冲区，要传播给从节点的命令会先写入该缓冲区，随后在通过长连接发送给从节点。</li>
<li><strong>repl_backlog_buffer</strong>：最近传播命令缓冲区，一个主节点只有一个，用于记录主节点最近传播出去的命令，主节点和全部从节点都会分别在上面维护一个 offset，用于表示自己已经写入或读取的命令进度。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd8naQ"><img src="https://s21.ax1x.com/2025/03/17/pEd8naQ.png" alt="pEd8naQ.png"></a></p>
<p>这里 <code>replication_buffer</code> 实际上就是 IO 操作通常都会有的缓冲区，而 <code>repl_backlog_buffer</code> 则是为了解决从节点掉线重连后的增量同步问题。</p>
<h2 id="增量同步"><a href="#增量同步" class="headerlink" title="增量同步"></a>增量同步</h2><p><code>repl_backlog_buffer</code> 是主节点中一个比较特殊的缓冲区，和每个从节点都有一个 <code>replication_buffer</code> 不同，一个主节点只会有一个 <code>repl_backlog_buffer</code> 。</p>
<p><code>repl_backlog_buffer</code> 是一个固定大小的“环形”区域，当主节点写入数据时，它会使用 <code>master_offset</code> 记录自己当前已经写到哪个字节，对应的，从节点也会有一个 <code>slave_offset</code>表示从节点已经读到的哪个字节。当主节点写的数据超过缓冲区大小后，它将会覆盖最早写过的内容。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd8u5j"><img src="https://s21.ax1x.com/2025/03/17/pEd8u5j.png" alt="pEd8u5j.png"></a></p>
<p>因此，当从节点要与主节点进行同步时，仅需要在 psync 请求时给出自己的 <code>slave_offset</code> 即可，主节点将计算其与 <code>master_offset</code> 的差值：</p>
<ul>
<li>如果差值大于 <code>repl_backlog_buffer</code> 的大小，说明两者数据已经差了很多，那么需要重新进行全量同步。</li>
<li>如果查找小于  <code>repl_backlog_buffer</code> 的大小，说明数据差还在容许范围内，则主节点将返回 <code>CONTINUE</code> 响应，让从节点准备进行增量同步，并把 <code>repl_backlog_buffer</code> 中的差值部分写入 <code>replication_buffer</code> 并发送给从节点，让从节点把同步进度追上来。</li>
</ul>
<p>简单的来说，如果从节点最后一次读取的命令可以在上面找到，那么说明从节点的数据没有落后太多，因此可以增量同步，否则就需要进行全量同步。</p>
<blockquote>
<p>当主从链接断开后，从节点会重新发送 psync 请求向主节点要求同步数据，在 2.8 之前总是会使用全量同步，而在 2.8 及以后的版本，将会使用增量同步。</p>
</blockquote>
<p>综上，我们不难意识到，<strong>如果你的网络不太稳定，那么最好把 repl_backlog_buffer</strong> <strong>调大一些，这样可以尽可能的避免从节点掉线重连后需要频繁的进行全量同步。</strong></p>
<h2 id="主动切换导致数据丢失"><a href="#主动切换导致数据丢失" class="headerlink" title="主动切换导致数据丢失"></a>主动切换导致数据丢失</h2><p>当一个 Redis 节点作为从节点使用时，为了保证始终能够正确从主节点同步数据，因此它的 <code>maxmemory</code> 配置将不会生效。</p>
<p>这也就意味，<strong>当你的从节点的 maxmemory 小于主节点的 maxmemory 配置时，从节点可能会从主节点同步到超过其 maxmemory 配置的数据量。此时一旦发生主从切换，从节点就会意识到自己的使用内存已经超过最大内存了，因此会立刻开始进行内存淘汰</strong>。在这种情况下，节点可能会导致大量的数据丢失，并且在较长一段时间内处于不可用状态。</p>
<p>总而言之，当你使用主从时，记得要让主节点和从节点的配置始终保持一致，避免因为主从切换导致不可预料的后果。</p>
<h1 id="Sentinel哨兵"><a href="#Sentinel哨兵" class="headerlink" title="Sentinel哨兵"></a>Sentinel哨兵</h1><p>针对普通主从模式无法自动进行故障转移的问题，Redis 在 2.6 及以上版本提供了哨兵功能。</p>
<p>哨兵是一种运行在特殊模式下的 Redis 节点，它用于监控集群中的节点监控状态，并在主节点下线后实现自动故障转移。此外，为了避免单点故障问题，哨兵通常也会集群部署。</p>
<p>当启动一台哨兵时，它会连接到主节点，并向该主节点发送 <code>INFO</code> 命令获取从节点相关信息，并连接到从节点。在此后，哨兵将会定期向它监控所有节点发送 <code>INFO</code>，从而及时了解集群节点的变动情况。</p>
<p>此外，每个哨兵连接到主节点后，都会订阅主节点的 <code>__sentinel__:hello</code> 频道，并且每隔 2s 向该频道发送 hello 消息，该消息包含了自己的地址和端口信息。其他订阅了该频道的哨兵节点收到消息后，将会并确认对方的存在，并与其建立链接。</p>
<p>每个哨兵节点都会定期向主从节点以及其他哨兵节点发送 ping  请求。当有节点超时未响应时，它将认为该节点主观下线。如果这个节点恰好是主节点，那么它们还会向其他哨兵节点确认对方是否也认为该节点主观下线，当有指定数量的哨兵认为该节点主观下线后，哨兵会认为该节点客观下线，并进行故障转移。</p>
<p>在故障转移开始前，哨兵集群会选择出一个领头哨兵负责整个过程，领头哨兵将会依次根据从节点在配置文件中配置的 <code>slave-priority</code>优先级、节点的数据复制进度和 runID 选择出优先级最高的从节点作为新的主节点，并将其他从节点指定为它的从节点，以完成故障转移。</p>
<h2 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h2><p>Redis 默认提供的主从模式<strong>不具备故障转移的能力</strong>，当主节点宕机后，需要用户手动的进行主从切换，在这段时间内整个集群实际上处于不可用的状态。为了解决这个问题，Redis 在 2.6 及以上版本提供了哨兵模式。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd8wGR"><img src="https://s21.ax1x.com/2025/03/17/pEd8wGR.png" alt="pEd8wGR.png"></a></p>
<p>简单的来说，哨兵模式与普通的主从复制相比，新增了一类哨兵节点，它是一个特殊的 Redis 进程，哨兵节点本身不对外提供任何读写服务，它的作用是：</p>
<ul>
<li><strong>监控与通知</strong>：与所有节点保持心跳，监控它们的监控状况。</li>
<li><strong>自动故障转移</strong>：当主节点故障后，自动选举新的主节点并完成主从切换，实现自动故障转移。</li>
<li><strong>通知告警</strong>：用户可以通过哨兵节点的 API 订阅消息，从而在节点状态异常时获得通知，又或者在完成主从切换后得知新的主节点地址。</li>
</ul>
<p>为了避免哨兵节点单点故障，因此哨兵也需要集群部署。多个哨兵节点可以组成一个哨兵集群，集群中的哨兵节点除了监控主从集群中节点的健康状况外，还会监控其他哨兵节点的健康状态。</p>
<h2 id="哨兵集群搭建"><a href="#哨兵集群搭建" class="headerlink" title="哨兵集群搭建"></a>哨兵集群搭建</h2><p>当你手头已经有一个主从集群后，你可以启动哨兵节点去监控这个集群。</p>
<p>整个流程大概如下：</p>
<ol>
<li><strong>连接到主节点</strong>：哨兵节点根据配置文件，连接到指定的主节点；</li>
<li><strong>订阅频道</strong>：连接成功后，哨兵节点将会订阅主节点 <code>__sentinel__:hello</code> 的频道。每个节点都会以 2s 为间隔向该频道发送自己的端口和 IP 等信息。此时，若有其他哨兵节点也订阅了这个该频道（换而言之，就是也监控了这个主节点），则它们会通过收到的消息感知到新哨兵的加入，并与新哨兵建立连接；</li>
<li><strong>获取节点信息</strong>：当订阅频道后，哨兵节点将会每隔 10 秒向所有节点发送 <code>INFO</code>命令获取相关信息，比如主节点的从节点信息，节点的 IP 、端口号、 runID 和 offset 等。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd8rM6"><img src="https://s21.ax1x.com/2025/03/17/pEd8rM6.png" alt="pEd8rM6.png"></a></p>
<h2 id="故障检测"><a href="#故障检测" class="headerlink" title="故障检测"></a>故障检测</h2><p>当哨兵节点启动以后，每隔 1 秒，它将会向集群中的<strong>主从节点与其他的哨兵节点</strong>发送 ping 请求，以确认对方的健康状况。如果在指定的超时时间 <code>down-after-millisenconds</code> 内没有收到响应，那么它就会认为没有响应的那个节点已经掉线，将其标记为 <code>sdown</code>。这种这种下线状态称为“<strong>主观下线</strong>”。</p>
<p>如果下线的是从节点或者其他哨兵节点，那么直到其重新上线为止，哨兵都会认为其已经不可用。如果下线的是<strong>主节点</strong>，那么哨兵为了防止因为网络波动导致的误判，会向其他的哨兵节点发送 <code>sentinel is-master-down-by-addr</code> 请求，确认对方是否也认定该节点下线。当整个哨兵集群中有足够数量的哨兵（该值通过配置文件中的 <code>quorum</code> 进行配置）节点确认主节点主观下线，那么主节点就会被认定为“<strong>客观下线</strong>”，并标记为 <code>odown</code>。</p>
<h2 id="故障转移"><a href="#故障转移" class="headerlink" title="故障转移"></a>故障转移</h2><h3 id="领头哨兵的选举"><a href="#领头哨兵的选举" class="headerlink" title="领头哨兵的选举"></a>领头哨兵的选举</h3><p>在一切开始前，哨兵集群会从所有的哨兵节点中挑选一个领头哨兵，<strong>领头哨兵将用来代表其他节点完成重新选主和主从切换的任务</strong>。</p>
<p>Redis 基于 Raft 分布式共识算法来实现领头哨兵的选举，这个过程大致如下：</p>
<ol>
<li>确认主节点客观下线的哨兵将成为候选者，它先投自己一票，然后邀请其他哨兵向自己投票；</li>
<li>其他哨兵如果尚未投票，则会将把赞同票投给邀请它投票的候选哨兵，否则就不投票；</li>
<li>所有的候选哨兵都统计自己的得票数，当得票数满足下述两条件时，该节点当选为领头哨兵：<ul>
<li>当得票数超过哨兵集群中节点数量的一半，即 <code>n/2+1</code>；</li>
<li>得票数且超过了认定节点客观下线所要求的哨兵数量（即配置文件中的 <code>quorum</code> 配置）；</li>
</ul>
</li>
<li>如果在规定时间内或投票结束后都没有选出领头哨兵，则再进行一轮投票，直到选出领头哨兵为止。</li>
</ol>
<p>由于选举机制需要保证有一个节点可以拿到超半数票，因此为了保证哨兵集群的高可用，<strong>一个最小的哨兵集群至少需要有三个节点</strong>，这样当任意一个节点下线后，哨兵集群仍然可以正常选举出领头哨兵去做故障转移。</p>
<p>此外，我们可以注意到，由于故障检测只受 <code>quorum</code> 影响，而不受哨兵数量的影响。因此，当 <code>quorum</code> 被设置了一个过小的值 —— 比如为低于 <code>n/2+1</code>——的时候，可能会出现虽然哨兵确认节点客观下线了，但是却由于在线的哨兵数量不足以选举出领头哨兵，最终无法进行故障转移尴尬局面。</p>
<p>比如，我们有一个由五个节点组成的哨兵集群，当我们把 <code>quorum</code> 设置为 2 时，如果挂掉了三个哨兵节点，此时哨兵集群可以确认主节点已经客观下线，但是由于哨兵节点数量不足，无法选出领头哨兵，所以即使发现了问题也无法完成故障转移。</p>
<h3 id="主从切换"><a href="#主从切换" class="headerlink" title="主从切换"></a>主从切换</h3><p>选举出领头哨兵后，它将从被自己监控的所有从节点中，先<strong>排除</strong>下述节点：</p>
<ul>
<li>排除所有已经掉线的从节点；</li>
<li>排除五秒内没有响应哨兵的 <code>INFO</code> 命令的从节点（当出现主节点客观下线后，哨兵对节点的 <code>INFO</code> 命令已经变为一秒请求一次）。</li>
<li>排除与主节点断连超过 <code>down-after-milliseconds * 10</code> 毫秒的从节点（<code>down-after-milliseconds</code>即为主节点的超时时间）。</li>
</ul>
<p>然后剩下的节点都有资格参与选举，此时哨兵再根据以下条件<strong>选择</strong>出优先级最高的节点作为新的主节点：</p>
<ol>
<li><strong>优先级配置</strong>：先尝试根据 <code>slave-priority</code> 配置，选出一个优先级最高的节点；</li>
<li><strong>数据同步进度</strong>：如果所有节点优先级配置均相同，则从中挑选出一个复制进度最高的（即主从复制中提到的从节点的 offset）；</li>
<li><strong>启动时间</strong>：如果所有的节点复制进度均相同，则挑选一个 runID 最小的节点（即最新的从节点）。</li>
</ol>
<p>当选出一个新的主节点后，领头哨兵将会发送 <code>slave no one</code> 命令将该节点真正的升级为主节点，然后向其他的从节点发送 <code>slaveof</code> 命令让它们成为这个新主节点的从节点，至此，故障转移的就完成了。</p>
<p>需要注意的是，在这个过程中，由于主从复制延迟，当主从切换后，旧的主节点尚未同步从节点的数据可能就会丢失。</p>
<h2 id="如何防止脑裂"><a href="#如何防止脑裂" class="headerlink" title="如何防止脑裂"></a>如何防止脑裂</h2><p>在正常情况下，当发生主从切换时，客户端将会从哨兵收到主节点切换的通知，然后与旧的主节点断开连接，并连接到新的主节点。不过，如果有的客户端与哨兵也断开了连接，那么它将无法意识到主节点已经切换，并且还是连接到旧的主节点。此时，整个 Redis 集群实际上同时有两个主节点在工作，这就是脑裂问题。</p>
<p>当出现脑裂问题时，由于有的客户端连接的是已下线的主节点，而有的客户端连接到的是新的主节点，因此两个节点的数据将会不一致。当已下线的主节点重新连接到集群后，如果将该节点降级为从节点，那么在进行数据同步时，从它下线以后到现在重新上线这段时间的数据就会全部丢失。</p>
<p>为此，我们可以通过调整两个参数防止这种情况出现：</p>
<ul>
<li><code>min-replicas-to-write</code>：主节点最少要有多少个可用的从节点。</li>
<li><code>min-replicas-max-lag</code>：如果主节点要允许客户端访问，那么它和它的从节点最大的复制延迟时间。</li>
</ul>
<p>简单的来说，这两个参数可以用于规定，如果主节点如果要对客户端提供服务，那它至少得有几个可用的从节点，并且它和它的从节点之间的复制延迟不大于多少秒。</p>
<p>如此一来，当主节点掉线时，我们就可以让客户端能够及时发现问题，然后重新向哨兵确认当前的主节点。在最大限度的避免的数据不一致问题后，当旧的主节点节点重新上线，就可以放心的让它去当从节点，而不必担心数据丢失的问题。</p>
<p>不过，有得必有失，为了避免主节点“罢工”，<code>min-replicas-to-write</code> 调的越高，从节点就需要部署的越多，<code>min-replicas-max-lag</code> 调的越小，那主从节点间的网络就要越稳定。因此，实际使用时，还是需要结合自己的业务需求来衡量要采用什么样的配置。</p>
<h1 id="Cluster集群"><a href="#Cluster集群" class="headerlink" title="Cluster集群"></a>Cluster集群</h1><h2 id="为什么需要Cluster集群"><a href="#为什么需要Cluster集群" class="headerlink" title="为什么需要Cluster集群"></a>为什么需要Cluster集群</h2><p>的确，哨兵在<strong>数据</strong>上，有replication副本做保证；<strong>可用性</strong>上，master宕机会自动的执行failover。首先Redis Sentinel说白了也是基于<strong>主从复制</strong>，在主从复制中slave的数据是完全来自于master。假设master节点的内存只有4G，那slave节点所能存储的数据上限也只能是4G。主从复制架构中是读写分离的，我们可以通过增加slave节点来扩展主从的读并发能力，但是<strong>写能力</strong>和<strong>存储能力</strong>是无法进行扩展的，就只能是master节点能够承载的上限。所以，当你只需要存储4G的数据时候的，基于主从复制和基于Sentinel的高可用架构是完全够用的。</p>
<p>但是如果当你面临的是海量的数据的时候呢？16G、64G、256G甚至1T呢？现在互联网的业务里面，如果你的体量足够大，我觉得是肯定会面临缓存<strong>海量</strong>缓存数据的场景的。</p>
<p>这就是为什么我们需要引入<strong>Redis Cluster</strong>。</p>
<h2 id="Cluster集群是什么"><a href="#Cluster集群是什么" class="headerlink" title="Cluster集群是什么"></a>Cluster集群是什么</h2><p>知道了为什么需要Redis Cluster之后，我们就可以来对其一探究竟了。</p>
<blockquote>
<p> 那什么是Redis Cluster呢？</p>
</blockquote>
<p>很简单，你就可以理解为n个主从架构组合在一起对外服务。Redis Cluster要求至少需要3个master才能组成一个集群，同时每个master至少需要有一个slave节点。</p>
<p>这样一来，如果一个主从能够存储32G的数据，如果这个集群包含了两个主从，则整个集群就能够存储64G的数据。</p>
<p>我们知道，主从架构中，可以通过增加slave节点的方式来扩展读请求的并发量，那Redis Cluster中是如何做的呢？虽然每个master下都挂载了一个slave节点，但是在Redis Cluster中的读、写请求其实都是在<strong>master</strong>上完成的。</p>
<p>slave节点只是充当了一个数据备份的角色，当master发生了宕机，就会将对应的slave节点提拔为master，来重新对外提供服务。</p>
<h2 id="节点负载均衡"><a href="#节点负载均衡" class="headerlink" title="节点负载均衡"></a>节点负载均衡</h2><p>这么多的master节点。我存储的时候，到底该选择哪个节点呢？一般这种负载均衡算法，会选择<strong>哈希算法</strong>。哈希算法是怎么做的呢？</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEdw2o6"><img src="https://s21.ax1x.com/2025/03/18/pEdw2o6.png" alt="pEdw2o6.png"></a></p>
<p>首先就是对key计算出一个hash值，然后用哈希值对master数量进行取模。由此就可以将key负载均衡到每一个Redis节点上去。这就是简单的<strong>哈希算法</strong>的实现。</p>
<p>那Redis Cluster是采取的上面的哈希算法吗？答案是<strong>没有</strong>。</p>
<p>Redis Cluster其实采取的是类似于<strong>一致性哈希</strong>的算法来实现节点选择的。那为什么不用哈希算法来进行实例选择呢？以及为什么说是类似的呢？我们继续讨论。</p>
<p>因为如果此时某一台master发生了宕机，那么此时会导致Redis中<strong>所有的缓存失效</strong>。为什么是所有的？假设之前有3个master，那么之前的算法应该是 hash % 3，但是如果其中一台master宕机了，则算法就会变成 hash % 2，会影响到之前存储的所有的key。而这对缓存后面保护的DB来说，是致命的打击。</p>
<h3 id="什么是一致性哈希"><a href="#什么是一致性哈希" class="headerlink" title="什么是一致性哈希"></a>什么是一致性哈希</h3><p>知道了通过传统哈希算法来实现对节点的负载均衡的弊端，我们就需要进一步了解<strong>什么是一致性哈希</strong>。</p>
<p>我们上面提过哈希算法是对master实例数量来取模，而<strong>一致性哈希</strong>则是对2^32取模，也就是值的范围在[0, 2^32 -1]。一致性哈希将其范围抽象成了一个圆环，使用CRC16算法计算出来的哈希值会落到圆环上的某个地方。</p>
<p>然后我们的Redis实例也分布在圆环上，我们在圆环上按照顺时针的顺序找到第一个Redis实例，这样就完成了对key的节点分配。我们举个例子。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEdwWFK"><img src="https://s21.ax1x.com/2025/03/18/pEdwWFK.png" alt="pEdwWFK.png"></a></p>
<p>假设我们有A、B、C三个Redis实例按照如图所示的位置分布在圆环上，此时计算出来的hash值，取模之后位置落在了<strong>位置D</strong>，那么我们按照顺时针的顺序，就能够找到我们这个key应该分配的Redis实例B。同理如果我们计算出来位置在E，那么对应选择的Redis的实例就是A。</p>
<p>即使这个时候Redis实例B挂了，也不会影响到实例A和C的缓存。那之前计算出来在位置D的key，此时会按照顺时针的顺序，找到节点C。相当于自动的把原来节点B的流量给转移到了节点C上去。而其他原本就在节点A和节点C的数据则完全不受影响。</p>
<p>这就是一致性哈希，能够在我们后续需要新增节点或者删除节点的时候，不影响其他节点的正常运行。</p>
<h3 id="虚拟节点机制"><a href="#虚拟节点机制" class="headerlink" title="虚拟节点机制"></a>虚拟节点机制</h3><p>但是一致性哈希也存在自身的小问题，例如当我们的Redis节点分布如下时，就有问题了。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEdwfJO"><img src="https://s21.ax1x.com/2025/03/18/pEdwfJO.png" alt="pEdwfJO.png"></a></p>
<p>此时数据落在节点A上的概率明显是大于其他两个节点的，其次落在节点C上的概率最小。这样一来会导致整个集群的数据存储不平衡，AB节点压力较大，而C节点资源利用不充分。为了解决这个问题，一致性哈希算法引入了<strong>虚拟节点机制</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEdw5Se"><img src="https://s21.ax1x.com/2025/03/18/pEdw5Se.png" alt="pEdw5Se.png"></a></p>
<p>在圆环中，增加了对应节点的虚拟节点，然后完成了虚拟节点到真实节点的映射。假设现在计算得出了位置D，那么按照顺时针的顺序，我们找到的第一个节点就是<strong>C #1</strong>，最终数据实际还是会落在节点C上。</p>
<p>通过增加虚拟节点的方式，使ABC三个节点在圆环上的位置更加均匀，平均了落在每一个节点上的概率。这样一来就解决了上文提到的数据存储存在不均匀的问题了，这就是一致性哈希的虚拟节点机制。</p>
<h2 id="Redis-Cluster采用了什么算法"><a href="#Redis-Cluster采用了什么算法" class="headerlink" title="Redis Cluster采用了什么算法"></a>Redis Cluster采用了什么算法</h2><p>上面提到过，Redis Cluster采用的是类一致性哈希算法，之所以是<strong>类一致性哈希算法</strong>是因为它们实现的方式还略微有差别。</p>
<p>例如一致性哈希是对2^32取模，而Redis Cluster则是对2^14（也就是16384）取模。Redis Cluster将自己分成了16384个<strong>Slot</strong>（槽位）。通过CRC16算法计算出来的哈希值会跟16384取模，取模之后得到的值就是对应的槽位，然后每个Redis节点都会负责处理一部分的槽位，就像下表这样。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEdwoyd"><img src="https://s21.ax1x.com/2025/03/18/pEdwoyd.png" alt="pEdwoyd.png"></a></p>
<p>每个Redis实例会自己维护一份<strong>slot - Redis节点</strong>的映射关系，当你尝试在节点A上设置一个key时，如果根据CRC16算法计算出该key应该分配到由节点B维护的槽位上，节点A会返回一个重定向消息给客户端，告知它正确的节点地址（即节点B）。这是因为Redis集群设计为允许客户端直接与持有所需数据的节点进行通信，从而提高效率。</p>
<p>这意味着：</p>
<ol>
<li><strong>初次请求</strong>：你的请求首先发送到了节点A。</li>
<li><strong>重定向响应</strong>：节点A分析出实际负责处理这个key的应该是节点B，并返回一个MOVED响应（例如：<code>MOVED &lt;slot&gt; &lt;ip&gt;:&lt;port&gt;</code>），指示你应该连接到指定的节点B。</li>
<li><strong>重定向操作</strong>：客户端收到重定向指令后，应当自动重新向节点B发起请求来完成原定的操作。</li>
</ol>
<blockquote>
<h1 id="Cluster集群可以基于一致性哈希算法么？"><a href="#Cluster集群可以基于一致性哈希算法么？" class="headerlink" title="Cluster集群可以基于一致性哈希算法么？"></a>Cluster集群可以基于一致性哈希算法么？</h1><p>首先，由于数据要分片存储在不同的服务器节点上，因此当用户发起请求时，我们会需要一个哈希算法保证对某个 Key 的请求<strong>总是路由到指定的服务器节点</strong>。传统的哈希算法基于长度（也就是节点数量）进行取模，因此当扩容和缩容时会<strong>需要对大量的数据进行迁移</strong>。而一致性哈希算法则用于解决这个问题。</p>
<p>在一致性哈希中，规定了一个<strong>固定大小的哈希环，环上均匀分布有 2^32 个节点，服务器节点也分布在环上</strong>。当用户发起请求时，将会根据 Key 的哈希值计算出落点，若 Key 的落点没有对应的服务器节点，则顺时针寻找最近的服务器节点。当扩容时，仅需要对新节点与上一节点之间的数据进行迁移，而缩容亦同理。</p>
<p>一致性哈希的优点在于<strong>避免了传统哈希算法中扩容和缩容时的大规模数据迁移</strong>，但是它也有缺点，那就是当<strong>节点在环上分布不均匀就可能导致数据倾斜</strong>。因此，当节点掉线或缩容时，可能因为将全部数据压到下一节点而导致<strong>服务雪崩</strong>。为了解决这个问题，我们需要允许一个节点在环上映射出多个<strong>虚拟节点</strong>，这样就能尽可能的使请求均匀的打到不同的服务器上。</p>
<p>Redis 集群采用的哈希槽方案同样基于固定数量（2^14）的哈希槽实现，不过相对于一致性哈希，每个节点拥有的哈希槽是可以直接分配的：</p>
<ul>
<li>我们可以直接根据服务器的性能而为它们分配不同数量的哈希槽位，一致性哈希虽然也可以通过改变某个服务器对应虚拟节点的数量做到类似的效果，不过显然没有这么精准。</li>
<li>当扩容和缩容的时候，我们可以手动指定将哪些槽位转移到哪些节点，而一致性哈希则做不到类似的效果。</li>
</ul>
<p>总的来说，哈希槽在使用上比一致性哈希更加灵活，并且实现上也更加简单。</p>
</blockquote>
<h2 id="如何做到高可用"><a href="#如何做到高可用" class="headerlink" title="如何做到高可用"></a>如何做到高可用</h2><p>不知道你思考过一个问题没，如果Redis Cluster中的某个master节点挂了，它是如何保证集群自身的高可用的？如果这个时候我们集群需要扩容节点，它该负责哪些槽位呢？我们一个一个问题的来看一下。</p>
<h3 id="集群如何进行扩容"><a href="#集群如何进行扩容" class="headerlink" title="集群如何进行扩容"></a>集群如何进行扩容</h3><p>我们开篇聊过，Redis Cluster可以很方便的进行横向扩容，那当新的节点加入进来的时候，它是如何获取对应的slot的呢？</p>
<p>答案是通过<strong>reshard</strong>（重新分片）来实现。reshard可以将已经分配给某个节点的任意数量的slot迁移给另一个节点，在Redis内部是由redis-trib负责执行的。你可以理解为Redis其实已经封装好了所有的命令，而redis-trib则负责向获取slot的节点和被转移slot的节点发送命令来最终实现reshard。</p>
<p>假设我们需要向集群中加入一个D节点，而此时集群内已经有A、B、C三个节点了。</p>
<p>此时redis-trib会向A、B、C三个节点发送迁移出槽位的请求，同时向D节点发送准备导入槽位的请求，做好准备之后A、B、C这三个源节点就开始执行迁移，将对应的slot所对应的键值对迁移至目标节点D。最后redis-trib会向集群中所有主节点发送槽位的变更信息。</p>
<h3 id="高可用以及故障转移"><a href="#高可用以及故障转移" class="headerlink" title="高可用以及故障转移"></a>高可用以及故障转移</h3><p>简单来说，针对A节点，某一个节点认为A宕机了，那么此时是<strong>主观宕机</strong>。而如果集群内超过半数的节点认为A挂了， 那么此时A就会被标记为<strong>客观宕机</strong>。</p>
<p>一旦节点A被标记为了客观宕机，集群就会开始执行<strong>故障转移</strong>。其余正常运行的master节点会进行投票选举，从A节点的slave节点中选举出一个，将其切换成新的master对外提供服务。当某个slave获得了超过半数的master节点投票，就成功当选。</p>
<p>当选成功之后，新的master会执行<code>slaveof no one</code>来让自己停止复制A节点，使自己成为master。然后将A节点所负责处理的slot，全部转移给自己，然后就会向集群发<strong>PONG</strong>消息来广播自己的最新状态。</p>
<p>按照<strong>一致性哈希</strong>的思想，如果某个节点挂了，那么就会沿着那个圆环，按照顺时针的顺序找到遇到的第一个Redis实例。</p>
<p>而对于Redis Cluster，某个key它其实并不关心它最终要去到哪个节点，他只关心他最终落到哪个slot上，无论你节点怎么去迁移，最终还是只需要找到对应的slot，然后再找到slot关联的节点，最终就能够找到最终的Redis实例了。</p>
<h3 id="gossip协议"><a href="#gossip协议" class="headerlink" title="gossip协议"></a>gossip协议</h3><p>这就是Redis Cluster各个节点之间交换数据、通信所采用的一种协议，叫做<strong>gossip</strong>(流言)</p>
<p>Redis Cluster就是利用了gossip来实现自身的<strong>信息扩散</strong>的。那使用gossip具体是如何通信的呢？</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd0AYT"><img src="https://s21.ax1x.com/2025/03/18/pEd0AYT.png" alt="pEd0AYT.png"></a></p>
<p>很简单，就像图里这样。每个Redis节点<strong>每秒钟</strong>都会向其他的节点发送<strong>PING</strong>，然后被<strong>PING</strong>的节点会回一个<strong>PONG</strong>。</p>
<h4 id="gossip消息协议类型"><a href="#gossip消息协议类型" class="headerlink" title="gossip消息协议类型"></a>gossip消息协议类型</h4><p>Redis Cluster中，节点之间的消息类型有5种，分别是MEET、PING、PONG、FAIL和PUBLISH。这些消息分别传递了什么内容呢</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pEd0ZpF"><img src="https://s21.ax1x.com/2025/03/18/pEd0ZpF.png" alt="pEd0ZpF.png"></a></p>
<h4 id="优劣"><a href="#优劣" class="headerlink" title="优劣"></a>优劣</h4><p>既然Redis Cluster选择了gossip，那肯定存在一些gossip的优点，我们接下来简单梳理一下。</p>
<ul>
<li>扩展性：网路可以允许节点的任意增加和减少，新增加的节点状态最终会和其他节点一致</li>
<li>容错性：由于每个节点都持有一份完整元数据，所以任何节点宕机都不会影响gossip的运行</li>
<li>健壮性：与容错性类似，由于所有节点都持有数据，使得整个集群成为去中心化的设计。任何节点的状态都不会影响到服务器的运行</li>
<li>最终一致性：当有新的信息需要传递时，消息可以快速地发送到所有的节点，让所有节点都拥有最新的数据</li>
</ul>
<p>gossip可以在O(logN) 轮就可以将信息传播到所有的节点，为什么是O(logN)呢？因为每次ping，当前节点会带上自己的信息外加整个Cluster的1&#x2F;10数量的节点信息，一起发送出去。你可以简单的把这个模型抽象为：</p>
<blockquote>
<p> 你转发了一个特别有意思的文章到朋友圈，然后你的朋友们都觉得还不错，于是就一传十、十传百这样的散播出去了，这就是朋友圈的<strong>裂变传播</strong>。</p>
</blockquote>
<p>当然，gossip仍然存在一些缺点。例如消息可能最终会经过很多轮才能到达目标节点，而这可能会带来较大的延迟。同时由于节点会随机选出5个最久没有通信的节点，这可能会造成某一个节点同时收到n个重复的消息。</p>
<h1 id="为什么redis常规架构不适合海量请求"><a href="#为什么redis常规架构不适合海量请求" class="headerlink" title="为什么redis常规架构不适合海量请求"></a>为什么redis常规架构不适合海量请求</h1><p>Redis 可以以多种方式进行部署，具体的部署架构会根据应用场景、性能需求和可用资源等因素而有所不同，常用的 Redis 部署架构包括单机部署、主从复制、Sentinel 哨兵模式、Cluster 集群模式。</p>
<p>这里面能承载更多并发的是集群模式，得益于集群模式的分片、主从复制、弹性扩容这些功能可以将集群模式自动化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应用来说，近乎于是透明的。</p>
<p>所以，<strong>Redis Cluster 是非常适合构建中小规模 Redis 集群</strong>，这里的中小规模指的是，大概几个到几十个节点这样规模的 Redis 集群。</p>
<p><strong>但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计</strong>。Redis  的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。但是它的更新会有一些问题，比如说，集群加入了新节点，或者某个主节点宕机了，新的主节点被选举出来，这些情况下，都需要更新集群每一个节点上的映射关系表。</p>
<p>Redis Cluster 采用了一种去中心化的 <strong>流言 (Gossip) 协议</strong>  来传播集群配置的变化。这个协议的特点类似于八卦，比如说，我们上学的时候，班上谁和谁偷偷好上了，搞对象，那用不了一天，全班同学都知道了。咋知道的？张三看见了，告诉李四，李四和王小二特别好，又告诉了王小二，这样人传人，不久就传遍全班了。这个就是八卦协议的传播原理。</p>
<p>这个八卦协议它的好处是去中心化，这样部署和维护就更简单，也能避免中心节点的单点故障。<strong>八卦协议的缺点就是传播速度慢，并且是集群规模越大，传播的越慢</strong>。这个也很好理解，比如说，换成某两个特别出名的明星搞对象，即使是全国人民都很八卦，但要想让全国每一个人都知道这个消息，还是需要很长的时间。在集群规模太大的情况下，数据不同步的问题会被明显放大，还有一定的不确定性，如果出现问题很难排查。</p>
<h1 id="Redis如何应对海量访问请求"><a href="#Redis如何应对海量访问请求" class="headerlink" title="Redis如何应对海量访问请求"></a>Redis如何应对海量访问请求</h1><p>如果想让 Redis 应对海量流量访问，可以在客户端和 Redis 真实节点之间，加上一层代理服务。</p>
<p>具体流程如下：</p>
<ol>
<li><strong>负责在客户端和 Redis 节点之间转发请求和响应</strong>：客户端只和代理服务打交道，代理收到客户端的请求之后，再转发到对应的 Redis 节点上，节点返回的响应再经由代理转发返回给客户端；</li>
<li><strong>负责监控集群中所有 Redis 节点状态</strong>：如果发现有问题节点，及时进行主从切换；</li>
<li><strong>维护集群的元数据</strong>：这个元数据主要就是集群所有节点的主从信息，以及槽和节点关系映射表。</li>
</ol>
<p>像开源的 Redis 集群方案 twemproxy 和 Codis，都是这种架构的。另外像分库分表中间件 ShardingSphere Proxy 原理同样如此。</p>
<p>这个架构最大的优点是对客户端透明，在客户端视角来看，<strong>整个集群和一个超大容量的单节点 Redis 是一样的</strong>。并且，由于分片算法是代理服务控制的，扩容也比较方便，新节点加入集群后，直接修改代理服务中的元数据就可以完成扩容。</p>
<p>不过，这个架构的缺点也很突出，增加了一层代理转发，每次数据访问的链路更长了，必然会带来一定的性能损失。而且，代理服务本身又是集群的一个单点，当然，我们可以把代理服务也做成一个集群来解决单点问题。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Allimac</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/22/Redis-High-Availability-architecture/">http://example.com/2025/03/22/Redis-High-Availability-architecture/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">AllimacBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%85%AB%E8%82%A1/">八股</a><a class="post-meta__tags" href="/tags/Redis/">Redis</a><a class="post-meta__tags" href="/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/">高可用</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/22/Redis-hot-issues/" title="Redis-hot-issues"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Redis-hot-issues</div></div><div class="info-2"><div class="info-item-1">Redis为什么这么快 redis 官方早前发布过一套基准测试，在 Redis 服务连接数小于 1 万时，并发数量每秒可以达到 10-12 万左右。连接数在 3-6 万 时，也能支持每秒 5-6 万的并发。 我觉得 Redis 之所以操作这么快，主要有以下几方面原因：  从存储方式上看：Redis 是基于内存的数据库，而直接访问内存的速度要比访问磁盘高上几个数量级。这是 Redis 快最主要的原因。 从设计上看：Redis 在架构上采用了 IO 多路复用提高了资源利用率，通过多线程非阻塞式 IO 提高请求的处理效率  ，使用单线程执行大部分命令以避免上下文切换，部分重命令则允许异步执行，并且在设计上针对最底层的数据结构进行了精细的优化，以保证任何操作都具备尽可能低的复杂度。 从使用方式上看：Redis 的功能非常纯粹，用户直接面向经过精心设计的数据结构进行操作，因此效率极高，此外，用户还可以根据自己的业务场景采用最合适的数据结构，这也间接提高了操作效率。  基于内存操作Redis 是基于内存操作的数据库，这是它快的最根本原因。 一般情况下，计算机访问一次 SSD 硬盘大概需要...</div></div></div></a><a class="pagination-related" href="/2025/03/22/RedisDS/" title="RedisDS"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">RedisDS</div></div><div class="info-2"><div class="info-item-1">Redis相关的数据结构</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/22/RedisDS/" title="RedisDS"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-22</div><div class="info-item-2">RedisDS</div></div><div class="info-2"><div class="info-item-1">Redis相关的数据结构</div></div></div></a><a class="pagination-related" href="/2025/03/22/Redis-hot-issues/" title="Redis-hot-issues"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-22</div><div class="info-item-2">Redis-hot-issues</div></div><div class="info-2"><div class="info-item-1">Redis为什么这么快 redis 官方早前发布过一套基准测试，在 Redis 服务连接数小于 1 万时，并发数量每秒可以达到 10-12 万左右。连接数在 3-6 万 时，也能支持每秒 5-6 万的并发。 我觉得 Redis 之所以操作这么快，主要有以下几方面原因：  从存储方式上看：Redis 是基于内存的数据库，而直接访问内存的速度要比访问磁盘高上几个数量级。这是 Redis 快最主要的原因。 从设计上看：Redis 在架构上采用了 IO 多路复用提高了资源利用率，通过多线程非阻塞式 IO 提高请求的处理效率  ，使用单线程执行大部分命令以避免上下文切换，部分重命令则允许异步执行，并且在设计上针对最底层的数据结构进行了精细的优化，以保证任何操作都具备尽可能低的复杂度。 从使用方式上看：Redis 的功能非常纯粹，用户直接面向经过精心设计的数据结构进行操作，因此效率极高，此外，用户还可以根据自己的业务场景采用最合适的数据结构，这也间接提高了操作效率。  基于内存操作Redis 是基于内存操作的数据库，这是它快的最根本原因。 一般情况下，计算机访问一次 SSD 硬盘大概需要...</div></div></div></a><a class="pagination-related" href="/2025/03/15/A-First-Look-At-Zookeeper/" title="A-First-Look-At-Zookeeper"><img class="cover" src="https://s21.ax1x.com/2025/03/15/pEaaHz9.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-15</div><div class="info-item-2">A-First-Look-At-Zookeeper</div></div><div class="info-2"><div class="info-item-1">初探Zookeeper，一些关于Zookeeper的问题</div></div></div></a><a class="pagination-related" href="/2025/03/15/A-First-Look-At-RocketMQ/" title="A-First-Look-At-RocketMQ"><img class="cover" src="https://s21.ax1x.com/2025/03/15/pEaULE8.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-15</div><div class="info-item-2">A-First-Look-At-RocketMQ</div></div><div class="info-2"><div class="info-item-1">初探RocketMQ，一些关于RocketMQ的问题</div></div></div></a><a class="pagination-related" href="/2025/03/15/A-First-Look-At-Kafka/" title="A-First-Look-At-Kafka"><img class="cover" src="https://s21.ax1x.com/2025/03/15/pEaUHDP.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-15</div><div class="info-item-2">A-First-Look-At-Kafka</div></div><div class="info-2"><div class="info-item-1">初探Kafka，一些关于Kafka的问题</div></div></div></a><a class="pagination-related" href="/2025/03/22/BloomFilter/" title="BloomFilter"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-22</div><div class="info-item-2">BloomFilter</div></div><div class="info-2"><div class="info-item-1">布隆过滤器入门</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Allimac</div><div class="author-info-description">华丽的仓库存放着我简陋的思想</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">61</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/AillemaCc"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/AillemaCc" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="http://www.swindle.icu/#/Home" target="_blank" title="曾经的博客"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这里是小梦一场的大床</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#redis%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">redis如何实现高可用架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E4%BB%8E%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.</span> <span class="toc-text">主从模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sentinel%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">sentinel模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cluster%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text">cluster模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%96%B9%E6%A1%88"><span class="toc-number">1.4.</span> <span class="toc-text">其他方案</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">主从复制原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E9%87%8F%E5%90%8C%E6%AD%A5"><span class="toc-number">2.1.</span> <span class="toc-text">全量同步</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96runid-offset"><span class="toc-number">2.1.1.</span> <span class="toc-text">获取runid offset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BDRDB%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.2.</span> <span class="toc-text">加载RDB数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.3.</span> <span class="toc-text">获取增量数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E4%BC%A0%E6%92%AD"><span class="toc-number">2.2.</span> <span class="toc-text">命令传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5"><span class="toc-number">2.3.</span> <span class="toc-text">增量同步</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E5%8A%A8%E5%88%87%E6%8D%A2%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1"><span class="toc-number">2.4.</span> <span class="toc-text">主动切换导致数据丢失</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sentinel%E5%93%A8%E5%85%B5"><span class="toc-number">3.</span> <span class="toc-text">Sentinel哨兵</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">哨兵模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">3.2.</span> <span class="toc-text">哨兵集群搭建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%85%E9%9A%9C%E6%A3%80%E6%B5%8B"><span class="toc-number">3.3.</span> <span class="toc-text">故障检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">3.4.</span> <span class="toc-text">故障转移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%86%E5%A4%B4%E5%93%A8%E5%85%B5%E7%9A%84%E9%80%89%E4%B8%BE"><span class="toc-number">3.4.1.</span> <span class="toc-text">领头哨兵的选举</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2"><span class="toc-number">3.4.2.</span> <span class="toc-text">主从切换</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E8%84%91%E8%A3%82"><span class="toc-number">3.5.</span> <span class="toc-text">如何防止脑裂</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cluster%E9%9B%86%E7%BE%A4"><span class="toc-number">4.</span> <span class="toc-text">Cluster集群</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81Cluster%E9%9B%86%E7%BE%A4"><span class="toc-number">4.1.</span> <span class="toc-text">为什么需要Cluster集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cluster%E9%9B%86%E7%BE%A4%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">4.2.</span> <span class="toc-text">Cluster集群是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">4.3.</span> <span class="toc-text">节点负载均衡</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C"><span class="toc-number">4.3.1.</span> <span class="toc-text">什么是一致性哈希</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E8%8A%82%E7%82%B9%E6%9C%BA%E5%88%B6"><span class="toc-number">4.3.2.</span> <span class="toc-text">虚拟节点机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Redis-Cluster%E9%87%87%E7%94%A8%E4%BA%86%E4%BB%80%E4%B9%88%E7%AE%97%E6%B3%95"><span class="toc-number">4.4.</span> <span class="toc-text">Redis Cluster采用了什么算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cluster%E9%9B%86%E7%BE%A4%E5%8F%AF%E4%BB%A5%E5%9F%BA%E4%BA%8E%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E4%B9%88%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">Cluster集群可以基于一致性哈希算法么？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">5.1.</span> <span class="toc-text">如何做到高可用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%89%A9%E5%AE%B9"><span class="toc-number">5.1.1.</span> <span class="toc-text">集群如何进行扩容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E4%BB%A5%E5%8F%8A%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="toc-number">5.1.2.</span> <span class="toc-text">高可用以及故障转移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gossip%E5%8D%8F%E8%AE%AE"><span class="toc-number">5.1.3.</span> <span class="toc-text">gossip协议</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#gossip%E6%B6%88%E6%81%AF%E5%8D%8F%E8%AE%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">5.1.3.1.</span> <span class="toc-text">gossip消息协议类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8A%A3"><span class="toc-number">5.1.3.2.</span> <span class="toc-text">优劣</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88redis%E5%B8%B8%E8%A7%84%E6%9E%B6%E6%9E%84%E4%B8%8D%E9%80%82%E5%90%88%E6%B5%B7%E9%87%8F%E8%AF%B7%E6%B1%82"><span class="toc-number">6.</span> <span class="toc-text">为什么redis常规架构不适合海量请求</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Redis%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E6%B5%B7%E9%87%8F%E8%AE%BF%E9%97%AE%E8%AF%B7%E6%B1%82"><span class="toc-number">7.</span> <span class="toc-text">Redis如何应对海量访问请求</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/12306Business-2/" title="12306Business-2">12306Business-2</a><time datetime="2025-04-22T10:14:27.000Z" title="发表于 2025-04-22 18:14:27">2025-04-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/12306Business-1/" title="12306Business-1">12306Business-1</a><time datetime="2025-04-22T10:10:07.000Z" title="发表于 2025-04-22 18:10:07">2025-04-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/12306Impl-4/" title="12306Impl-4">12306Impl-4</a><time datetime="2025-04-22T10:06:14.000Z" title="发表于 2025-04-22 18:06:14">2025-04-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/12306Impl-3/" title="12306Impl-3">12306Impl-3</a><time datetime="2025-04-22T10:03:30.000Z" title="发表于 2025-04-22 18:03:30">2025-04-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/22/12306Impl-2/" title="12306Impl-2">12306Impl-2</a><time datetime="2025-04-22T10:00:54.000Z" title="发表于 2025-04-22 18:00:54">2025-04-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Allimac</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>